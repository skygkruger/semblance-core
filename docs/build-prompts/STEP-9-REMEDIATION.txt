Step 9 audit FAILED on 3 critical exit criteria. The embedding pipeline, hardware detection, onboarding UI, and settings are all good — do not touch those. The following must be completed:
1. Commit 3 was never finished. llama-cpp-2 is not in Cargo.toml. native_runtime.rs is stubbed. Add the dependency, implement real model loading and inference via llama-cpp-2 with Metal backend on macOS and CPU fallback. This is the highest-risk work — if compilation fails, escalate immediately with the specific error. Do not fake it with placeholder returns.
2. Commit 8 was never finished. The InferenceRouter exists but is not wired into SemblanceCore or the Orchestrator. packages/core/llm/index.ts still returns new OllamaProvider(). Replace the default provider path with InferenceRouter. Every caller identified in the implementation plan must route through the router. The guard test must verify this — if the guard test currently passes while direct calls remain, the guard test is broken. Fix it.
3. NativeProvider must actually call the NativeRuntime. The bridge callback mechanism exists but the Rust side returns errors. Wire the callback handler in lib.rs to the real NativeRuntime once it's implemented.
Do not add new tests for capabilities that already pass. Fix the implementations that are stubbed. Exit criteria 1, 8, and 10 must pass for real, not against mocks or stubs.
If llama-cpp-2 Rust compilation fails: This is the documented escalation scenario. Report the exact error, the platform, and what you tried. Do not work around it silently. The fallback (managed llama-server subprocess) requires Orbital Director approval.